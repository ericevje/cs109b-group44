{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "967e2d4c",
   "metadata": {},
   "source": [
    "# Design: Learning Object Construction Sequences\n",
    "#### Erik Beerepoot, Eric Evje, Eagon Meng, Marcos Rios\n",
    "#### 04/10/21"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9c6c21",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Fusion 360 is a computer aided design (CAD) software developed by Autodesk. This program and others like it are used by designers in many fields to design solid models from parametric sequences of more basic shapes and extrusions. When these models are saved in a software specific file format, the steps taken to construct the final shape are preserved. However, if a file has to change software packages, the original steps were lost due to saving as a file type that does not support, or if the solid model is made from a reconstruction of an arbitrary object 3D scanned into a computer, then these steps are lost or never existed in the first place. There is a desire to be able to take a solid model representation and automatically reconstruct the more basic steps taken to construct the model. Having the constituent steps makes editing the model easier by designers after this step, and could save time from reproducing the models by hand. \n",
    "\n",
    "Developers at Autodesk used the $Fusion 360 Gallery Dataset$, a collection of solid models and files that preserve the construction and 3D representation information of target models. They trained an imitation learning model on these steps, embedding the Q table in a message passing network (MPN) and developed a custom search algorithm to take in a target object, and the current state of reconstruction, allow an agent to step through developing the parametric steps needed to reconstruct the model exactly. They were successful, but there was room for improvement. \n",
    "\n",
    "We decided to augment their existing codebase to attempt and improve the functionaity and performance of their learning environment. We chose to develop and test two possible improvements, detailed in the notebook below. These improvements were:\n",
    "\n",
    "- Modifying the network architecture to improve the MPN, both by increasing the depth and width of the deep neural network as well as connecting output layers more fully to attempt to boost performance. These efforts were relatively successfull and detailed in the `Modifying the network architecture` section. \n",
    "- Modifying the search function to search for additive steps to reach the target shape, as well as sequentially performing substrative steps from a bounding box shape to reach the target shape, then choose the reconstruction with higher intersection of union (IoU). This effort was unsuccessful but gave insight into future work that could expand on this. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522ef3a4",
   "metadata": {},
   "source": [
    "# Modifying the network architecture\n",
    "\n",
    "### Overview\n",
    "\n",
    "We made some changes to the model architecture to improve model performance. We made two modifications: First, we added the operation as an input to the MLPs predicting the start and end faces. The hypothesis was that depending on the type of operation being performed (say, a cut vs an additive operation), the prediction for start and end might be significantly different.\n",
    "\n",
    "  Second, we increased both the width and depth of the graph convolution network (henceforth, GCN), the width of the fully connected layers, decreased the learning rate and increased the number of epochs. The original GCN contained only two layers: the input and output layers. As explained in the course material, the inner layers of CNNs will learn progressively higher level features (e.g, lines, shapes, etc). The intent behind the increased depth was to allow the network to learn better representations of the input graphs. The width of the hidden layers of the MLPs were increased as there were already several fully dense layers (i.e. adequate depth). The definition of the model is shown below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4c7881",
   "metadata": {},
   "source": [
    "```python\n",
    "class DeepGCN(nn.Module):\n",
    "    \"\"\" DeepGCN: A modified Graph Convolution network \n",
    "    with more layers \"\"\"\n",
    "    def __init__(self, nfeat, nhid, dropout):\n",
    "        super(DeepGCN, self).__init__()\n",
    "\n",
    "        self.gc1 = GraphConvolution(nfeat, nhid)\n",
    "        self.gc2 = GraphConvolution(nhid, nhid)\n",
    "        self.gc3 = GraphConvolution(nhid, nhid)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = F.relu(self.gc1(x, adj))\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = F.relu(self.gc2(x, adj))\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = F.relu(self.gc3(x, adj))\n",
    "        return x\n",
    "\n",
    "\n",
    "class AugmentedNodePointer(nn.Module):\n",
    "    \"\"\" AugmentedNodePointer: improved model that feeds the predicted \n",
    "    operation back into dense layers to predict start & end faces.\n",
    "    \"\"\"\n",
    "    def __init__(self, nfeat, nhid, dropout=0.0, Use_GCN=True):\n",
    "        super(NodePointer,self).__init__()\n",
    "        self.Use_GCN=Use_GCN\n",
    "        self.nhid=nhid\n",
    "\n",
    "        # \"embedding\" layers\n",
    "        if Use_GCN:\n",
    "            self.fc00=nn.Linear(nfeat,nhid)\n",
    "            self.fc01=nn.Linear(nhid,nhid)\n",
    "            self.fc10=nn.Linear(nfeat,nhid)\n",
    "            self.fc11=nn.Linear(nhid,nhid)\n",
    "            self.fc20=nn.Linear(nfeat*2,nhid)\n",
    "            self.fc21=nn.Linear(nhid,nhid)\n",
    "            self.gcn0=DeepGCN(nfeat=nhid,nhid=nhid,dropout=dropout)\n",
    "            self.gcn1=DeepGCN(nfeat=nhid,nhid=nhid,dropout=dropout)\n",
    "            self.gcn2=DeepGCN(nfeat=nhid,nhid=nhid,dropout=dropout)\n",
    "            self.fc02=nn.Linear(nhid,nhid)\n",
    "            self.fc03=nn.Linear(nhid,nhid)\n",
    "            self.fc12=nn.Linear(nhid,nhid)\n",
    "            self.fc13=nn.Linear(nhid,nhid)\n",
    "            self.fc22=nn.Linear(nhid,nhid)\n",
    "            self.fc23=nn.Linear(nhid,nhid)\n",
    "        else:\n",
    "            self.fc00=nn.Linear(nfeat,nhid)\n",
    "            self.fc01=nn.Linear(nhid,nhid)\n",
    "            self.fc10=nn.Linear(nfeat,nhid)\n",
    "            self.fc11=nn.Linear(nhid,nhid)\n",
    "            self.fc20=nn.Linear(nfeat*2,nhid)\n",
    "            self.fc21=nn.Linear(nhid,nhid)\n",
    "\n",
    "        # \"output\" layers        \n",
    "        self.fc_operation=nn.Linear(nhid,5)\n",
    "\n",
    "        # feed in operation (augmented model)\n",
    "        self.fc0=nn.Linear(nhid*2 + 5,nhid*2)\n",
    "        self.fc1=nn.Linear(nhid*2,nhid*2)\n",
    "        self.fc2=nn.Linear(nhid*2,nhid*2)\n",
    "        self.fc3=nn.Linear(nhid*2,nhid*2)\n",
    "        self.fc_start=nn.Linear(nhid*2,1)\n",
    "        # feed in operation augmented model)\n",
    "        self.fc4=nn.Linear(nhid  + 5,nhid)\n",
    "        self.fc5=nn.Linear(nhid,nhid)\n",
    "        self.fc6=nn.Linear(nhid,nhid)\n",
    "        self.fc7=nn.Linear(nhid,nhid)\n",
    "        self.fc_end=nn.Linear(nhid,1)\n",
    "        \n",
    "        for m in self.modules():\n",
    "            if isinstance(m,nn.Linear):\n",
    "                torch.nn.init.xavier_uniform_(m.weight)\n",
    "                m.bias.data.fill_(0.00)\n",
    "\n",
    "    def forward(self, gpf, use_gpu):\n",
    "        \"\"\" Perform a forward pass \n",
    "        gpf - formatted graph pair, defined as:\n",
    "        [\n",
    "            target graph (adjacency matrix)\n",
    "            target features [{surface type, points, normals, trimming mask, edges}]\n",
    "                - for each face, we have a dict of the above features\n",
    "            current graph (adjacency matrix)\n",
    "            current features [{surface type, points, normals, trimming mask, edges}]\n",
    "                - for each face, we have a dict of the above features\n",
    "            start face\n",
    "            end face\n",
    "            operation\n",
    "            sequence name (e.g. 55928_1ccd0821_0001)\n",
    "            sequence id \n",
    "        ]\n",
    "        use_gpu - True if GPU should be used\n",
    "        \"\"\"\n",
    "        \n",
    "        # Using the above definition, we have\n",
    "        # gpf[1] => features of all faces\n",
    "        # ,gpf[1][gpf[4],:].repeat => features of the starting face repeated the match shape\n",
    "        x2=torch.cat((gpf[1],gpf[1][gpf[4],:].repeat(gpf[1].size()[0],1)),dim=1)\n",
    "        if self.Use_GCN:\n",
    "            # Combine target graph with features\n",
    "            x0=F.relu(self.fc01(F.relu(self.fc00(gpf[1]))))\n",
    "            x0=self.gcn0(x0,gpf[0])\n",
    "            x0=F.relu(self.fc03(F.relu(self.fc02(x0))))\n",
    "            \n",
    "            # Embed target graph with features of start face (repeated)\n",
    "            x2=F.relu(self.fc21(F.relu(self.fc20(x2))))\n",
    "            x2=self.gcn2(x2,gpf[0])\n",
    "            x2=F.relu(self.fc23(F.relu(self.fc22(x2))))\n",
    "            \n",
    "            # If the current graph is empty, this is the first step in the sequence\n",
    "            # so we set x1 to 0\n",
    "            if gpf[2].size()[0]==0:\n",
    "                if use_gpu:\n",
    "                    x1=torch.zeros((1,self.nhid)).cuda()\n",
    "                else:\n",
    "                    x1=torch.zeros((1,self.nhid))\n",
    "            # otherwise, embed current graph with features\n",
    "            else:\n",
    "                x1=F.relu(self.fc11(F.relu(self.fc10(gpf[3]))))\n",
    "                x1=self.gcn1(x1,gpf[2])\n",
    "                x1=F.relu(self.fc13(F.relu(self.fc12(x1))))\n",
    "        else:\n",
    "            # Same as above with graph nn\n",
    "            x0=F.relu(self.fc01(F.relu(self.fc00(gpf[1]))))\n",
    "            x2=F.relu(self.fc21(F.relu(self.fc20(x2))))\n",
    "            if gpf[2].size()[0]==0:\n",
    "                if use_gpu:\n",
    "                    x1=torch.zeros((1,self.nhid)).cuda()\n",
    "                else:\n",
    "                    x1=torch.zeros((1,self.nhid))\n",
    "            else:\n",
    "                x1=F.relu(self.fc11(F.relu(self.fc10(gpf[3]))))\n",
    "        \n",
    "        # NOTE: At this point we have obtained embeddings (Figure 5, left box)\n",
    "        # and now we proceed to pass them into a dense network\n",
    "        \n",
    "        # First, sum the current embedding (Figure 5, lower middle box)\n",
    "        x1=torch.sum(x1,dim=0,keepdim=True).repeat(x0.size()[0],1)\n",
    "        \n",
    "        # Predict the operation (dense nn with 5 outputs)\n",
    "        op=self.fc_operation(x1[0:1,:])\n",
    "        \n",
    "        # Use current & target embedding to predict start (single output)\n",
    "        # and add the operation as an input \n",
    "        x4 = op.repeat(x0.size()[0], 1)      \n",
    "        x=torch.cat((x0,x1,x4),dim=1)\n",
    "        x=F.relu(self.fc0(x))\n",
    "        x=F.relu(self.fc1(x))\n",
    "        x=F.relu(self.fc2(x))\n",
    "        x=F.relu(self.fc3(x))\n",
    "        x_start=self.fc_start(x)\n",
    "        \n",
    "        # Use target embedding with start face features to predict end (single output)\n",
    "        x=torch.cat((x2, x4), dim=1)\n",
    "        x2=F.relu(self.fc4(x))\n",
    "        x2=F.relu(self.fc5(x2))\n",
    "        x2=F.relu(self.fc6(x2))\n",
    "        x2=F.relu(self.fc7(x2))\n",
    "        x_end=self.fc_end(x2)\n",
    "        return x_start, x_end, op\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819c0f0d",
   "metadata": {},
   "source": [
    "## Results and Analysis\n",
    "\n",
    "Results from training the model are shown below. In the table, `original` refers to the original model, `augmented` refers to the model whereby the operation was fed into the MLP prediction start- and end faces, and `Augmented DeepGCN` refers to the deep-and-wide augmented model. Overall, we can see that that both changes yielded very slight improvements in accuracy. Note that the augmented DeepGCN model had the best training results by a wide margin. It appears that there is some overfitting, which could be mitigated by increasing dropout or increasing the amount of training data.\n",
    "\n",
    "### Train\n",
    "\n",
    "| Model  | Loss  | Start Accuracy  | End Accuracy  | Operation Accuracy | Overall Accuracy  |\n",
    "|---|---|---|---|---|---|\n",
    "| Original  | 0.46  | 92.59  |  96.24  | 96.52  | 88.17  |\n",
    "| Augmented  | 0.43  | 92.24  | 97.17 | 96.67 | 88.54  |\n",
    "| Augmented DeepGCN  |  0.18 |  95.27 | 98.94  | 99.25  | 94.18 |\n",
    "\n",
    "### Test\n",
    "| Model  | Loss  | Start Accuracy  | End Accuracy  | Operation Accuracy | Overall Accuracy  |\n",
    "|---|---|---|---|---|---|\n",
    "| Original  | 5.28  | 86.68  |  88.00  | 90.75  | 78.06  |\n",
    "| Augmented  | 4.06  | 86.05  | 88.71 | 91.37 | 78.37  |\n",
    "| Augmented DeepGCN  |  5.26 |  86.21 | 88.55  | 91.38  | 78.44 |\n",
    "\n",
    "\n",
    "Refer to [this notebook](https://colab.research.google.com/drive/10-rlj6X5XiySVLyOLruMxzxP78QALxT2?usp=sharing) to see the details on how the original model was re-trained.\n",
    "\n",
    "Refer to [this notebook](https://colab.research.google.com/drive/1TPnP0VzcImJmjIGrZ0jLlK9aQEhMzLXC?usp=sharing) to see the details on how the augmented models were trained."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4393fea",
   "metadata": {},
   "source": [
    "# Search Algorithm Augmentation: Subtractive Modeling Process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7450c1d2",
   "metadata": {},
   "source": [
    "The original `Reconstruction` search feature begins the search space with an empty graph, building up the body from `NewBodyFeatureOperation`, `JoinFeatureOperation`, `CutFeatureOperation`, or `IntersectFeatureOperation`. The search space is originally restricted to `NewBodyFeatureOperation` since the other operations are not possible on an empty graph. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b231e9",
   "metadata": {},
   "source": [
    "<p float=\"left\"> \n",
    "  <img src=\"images/best_search_1.png\" style=\"display:inline;margin:1px;height:300px\" /> \n",
    "  <img src=\"images/best_search_2.png\" style=\"display:inline;margin:1px;height:300px\" />  \n",
    "  <img src=\"images/best_search_3.png\" style=\"display:inline;margin:1px;height:300px\" /> \n",
    "</p>\n",
    "\n",
    "<center> Example face extrusion process for original best_search process </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b017d12f",
   "metadata": {},
   "source": [
    "## Simulating manufacturing processes in design\n",
    "\n",
    "### Overview\n",
    "\n",
    "There are 2 umbrellas of manufacturing for manufactured parts, additive manufacturing and subtractive manufacturing. Additive manufacturing, such as injection molding and 3D printing, *add* material to create a solid object. Opposite of this is machining, which starts with a block of materials and *subtracts* material to create the final design. Additive manufacturing is like working with clay, whereas subtractive manufacturing is like working with a granite block and chisel. \n",
    "\n",
    "Good design practices recommend designing your part in the way that it will be manufactured. If a part is to be manufactured via a subtractive manufacturing process, it is best practice to start your model as a block of material and cut away material. This allows the designer to visualize and optimize the design for the manufcaturing process and allows for easier editing of the design later on. \n",
    "\n",
    "The search feature implemented in the $Search$ class and its children start with a blank graph and *add* bodies to form the design. Though its possible for the agent to choose to extrude a bounding box then cut away to the target, it is not guaranteed, and typically not what happens. Further, due to the limitations imposed by the message passing network, namely the fact it returns 2 faces and an operation, as well the rules imposed on the agent (namely the faces must be parallel) there are several more complex shapes that it completely fails at making. Essentially, if there is a face with no parallel faces it canot be created. See examples below of a shape the agent fails to reconstruct. \n",
    "\n",
    "<p float=\"center\"> \n",
    "  <img src=\"images/77211_d46ae17d_0013.png\" style=\"height:300px\"/> \n",
    "</p>\n",
    "\n",
    "<center> Pyramidal face with no parallel faces fails to be reconstructed </center>\n",
    "\n",
    "Looking into the ground truth of the above model, it is apparent that the final model was formed from a bounding box, then two extrude cut operations were used to complete the shape. Since the final cuts go all the way through the model, there are no parallel faces for the agent to complete a face extrusion. \n",
    "\n",
    "<p float=\"center\"> \n",
    "  <img src=\"images/77211_d46ae17d_0013_0112.png\" style=\"display:inline;margin:1px;height:300px\" /> \n",
    "  <img src=\"images/77211_d46ae17d_0013_0116.png\" style=\"display:inline;margin:1px;height:300px\" />  \n",
    "  <img src=\"images/77211_d46ae17d_0013.png\" style=\"display:inline;margin:1px;height:300px\" /> \n",
    "</p>\n",
    "\n",
    "<center> Extrusion steps to recreate the solid object </center>\n",
    "\n",
    "This particular model fails the search algorithm due to the parallel face limitation of face extrusions. If we were able to start with the bounding box and cut away to reveal the model, however, this could be reconstructed using the existing framework of the search algorithm.\n",
    "\n",
    "### Implementation\n",
    "\n",
    "The full implementation can be found in the group's [github repo](https://github.com/ericevje/cs109b-group44/tree/eric-dev-2). To implement this, we needed to modify the search algorithm to add a bounding box before each face extrusion operation. The code below was added to the $search\\_best$ script used to search and update action probabilities based on the step or steps with best $IoU$. \n",
    "\n",
    "```python\n",
    "    def search_bounding_box(self, agent, budget, max_point, min_point, score_function=None, screenshot=False):\n",
    "        super().search_bounding_box(agent, budget, max_point, min_point, score_function, screenshot)\n",
    "        # the length of rollout is the same as the number of planar faces as a maximum\n",
    "        rollout_length = 0\n",
    "        for node in self.target_graph[\"nodes\"]:\n",
    "            if node[\"surface_type\"] == \"PlaneSurfaceType\":\n",
    "                rollout_length += 1\n",
    "        if rollout_length < 2:\n",
    "            # There exist some designs with no planar faces that we can't handle\n",
    "            # We need at least 2 faces\n",
    "            raise Exception(\"Not enough valid planar faces in target\")\n",
    "\n",
    "        used_budget = 0\n",
    "        max_score = 0\n",
    "        max_scores = []\n",
    "\n",
    "        fringe = PriorityQueue()\n",
    "        fringe.put(PriorityAction(0, ()))\n",
    "\n",
    "        # while there is item in the fridge and we still have budget\n",
    "        while fringe.qsize() > 0 and used_budget < budget:\n",
    "            # Revert environment to target and set current graph to bounding box\n",
    "\n",
    "            ###################################################################\n",
    "            # Instead of starting with a blank graph and adding, we start with\n",
    "            # a bounding box and subtract from it\n",
    "            self.env.revert_to_target()\n",
    "            cur_graph = self.env.get_bounding_graph(min_point, max_point)\n",
    "            ###################################################################\n",
    "            \n",
    "            priority_action = fringe.get()\n",
    "            # nll is something like 10, prefix is something like (a1, a4, a10)\n",
    "            nll = priority_action.nll\n",
    "            prefix = priority_action.prefix\n",
    "            new_graph, cur_iou = self.env.extrudes(list(prefix), revert=False)\n",
    "\n",
    "            if len(prefix) > 0:\n",
    "                used_budget += 1\n",
    "                take_screenshot = screenshot\n",
    "                if cur_iou is not None:\n",
    "                    max_score = max(max_score, cur_iou)\n",
    "                else:\n",
    "                    # We only want to take screenshots when something changes\n",
    "                    take_screenshot = False\n",
    "                if new_graph is not None:\n",
    "                    cur_graph = new_graph\n",
    "\n",
    "                log_data = {\n",
    "                    # \"rollout_attempt\": rollout_attempt,\n",
    "                    # \"rollout_step\": i,\n",
    "                    # \"rollout_length\": rollout_length,\n",
    "                    \"used_budget\": used_budget,\n",
    "                    \"budget\": budget,\n",
    "                    \"current_iou\": cur_iou,\n",
    "                    \"max_iou\": max_score,\n",
    "                    \"prefix\": list(prefix)\n",
    "                }\n",
    "                self.log.log(log_data, take_screenshot)\n",
    "                max_scores.append(max_score)\n",
    "                # Stop early if we find a solution\n",
    "                if math.isclose(max_score, 1, abs_tol=0.00001):\n",
    "                    return max_scores\n",
    "                # Stop if the rollout hits the budget\n",
    "                if used_budget >= budget:\n",
    "                    break\n",
    "            # If there was an invalid operation\n",
    "            # continue without adding it to the search space\n",
    "            if (new_graph is None or cur_iou is None) and len(prefix) > 0:\n",
    "                continue\n",
    "\n",
    "            # extend the current prefix by 1 step forward\n",
    "            actions, action_probabilities = agent.get_actions_probabilities(cur_graph, self.target_graph)\n",
    "            # Filter for clearly bad actions\n",
    "            action_probabilities = self.filter_bad_actions_bounded(cur_graph, actions, action_probabilities)\n",
    "            # Convert probability to logpr so they can be added rather than multiplied for numerical stability\n",
    "            action_logprs = np.log(action_probabilities)\n",
    "            # add to the candidates back to fringe\n",
    "            for (a, a_logpr) in zip(actions, action_logprs):\n",
    "                child_prefix = prefix + (a,)\n",
    "                child_nll = nll - a_logpr\n",
    "                # do not add a prefix that's longer than rollout length\n",
    "                if len(child_prefix) < rollout_length:\n",
    "                    fringe.put(PriorityAction(child_nll, child_prefix))\n",
    "\n",
    "            print(f\"[{used_budget}/{budget}] Score: {max_score}\")\n",
    "        return max_scores\n",
    "```\n",
    "\n",
    "We call the Fusion 360 environment and the custom function, `get_bounding_graph` which does the following:\n",
    "\n",
    "```python\n",
    "    def get_bounding_graph(self, min_point, max_point):\n",
    "        \"\"\"Wrapper to extrude a bounding rectangle around a solid model\"\"\"\n",
    "        _min_point = min_point\n",
    "        _max_point = max_point\n",
    "\n",
    "        start_face, end_face = self.boundary_points(_min_point, _max_point)\n",
    "        distance = _max_point[0] - _min_point[0]\n",
    "\n",
    "        # Create a new sketch vias the server client\n",
    "        r = self.client.add_sketch(\"XY\")\n",
    "        response_json = r.json()\n",
    "        sketch_name = response_json[\"data\"][\"sketch_name\"]\n",
    "\n",
    "        # Add points to new sketch\n",
    "        for i, point in enumerate(start_face):\n",
    "            r = self.client.add_point(sketch_name, \n",
    "                {\"x\": point[0], \"y\": point[1], \"z\":point[2]})\n",
    "            response_json = r.json()\n",
    "\n",
    "        # Close profile of points to make a face to extrude\n",
    "        r = self.client.close_profile(sketch_name)\n",
    "        response_json = r.json()\n",
    "        profile_id = (list(response_json['data']['profiles'].keys())[0])\n",
    "\n",
    "        # Extrude by distance between start and end face\n",
    "        r = self.client.add_extrude(sketch_name, profile_id, -1 * distance, \"NewBodyFeatureOperation\")\n",
    "\n",
    "        # Convert extrusion to graph and return graph format\n",
    "        r = self.client.graph(file='', dir='', format='PerFace')\n",
    "        response_json = r.json()\n",
    "        return response_json['data']['graph']\n",
    "```\n",
    "```python\n",
    "def boundary_points(self, min_point, max_point):\n",
    "        \"\"\"Return points needed to make a boundary extrude encompassing model\"\"\"\n",
    "        start_face = []\n",
    "        end_face = []\n",
    "        start_face.append(min_point)\n",
    "        start_face.append((min_point[0], min_point[1], max_point[2]))\n",
    "        start_face.append((min_point[0], max_point[1], max_point[2]))\n",
    "        start_face.append((min_point[0], max_point[1], min_point[2]))\n",
    "\n",
    "        end_face.append((max_point[0], min_point[1], min_point[2]))\n",
    "        end_face.append((max_point[0], min_point[1], max_point[2]))\n",
    "        end_face.append((max_point[0], max_point[1], max_point[2]))\n",
    "        end_face.append((max_point[0], max_point[1], min_point[2]))\n",
    "\n",
    "        return start_face, end_face\n",
    "```\n",
    "\n",
    "This creates a bounding box around the target object. Then, through the function `filter_bad_actions_bounded` we force the probabilities to near zero of any operation that is not a `CutFeatureOperation` operation as shown in the code snippet below:\n",
    "\n",
    "<p float=\"center\"> \n",
    "  <img src=\"images/bounding_box.gif\" style=\"height:200px\"/> \n",
    "</p>\n",
    "\n",
    "<center> GIF showing the bounding box extrusion step </center>\n",
    "\n",
    "```python\n",
    "def filter_bad_actions_bounded(self, current_graph, actions, action_probabilities):\n",
    "        \"\"\"Filter out some actions we clearly don't want to take\"\"\"\n",
    "        assert self.target_graph is not None\n",
    "        epsilon = 0.00000000001\n",
    "        # Adjust the probabilities of bad actions\n",
    "        for index, action in enumerate(actions):\n",
    "            if action[\"operation\"] == \"NewComponentFeatureOperation\":\n",
    "                action_probabilities[index] = epsilon\n",
    "            if action[\"operation\"] != \"CutFeatureOperation\":\n",
    "                action_probabilities[index] = epsilon\n",
    "            if action_probabilities[index] < epsilon:\n",
    "                action_probabilities[index] = epsilon\n",
    "\n",
    "        action_probabilities = action_probabilities / sum(action_probabilities)\n",
    "        return action_probabilities\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1265a8a9",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "9f4829ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "\n",
    "import sys\n",
    "import os\n",
    "from IPython.core.display import display, HTML, Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "720581d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "control_log = \"../tools/search/log/\"\n",
    "bounded_log = \"../tools/search/log_bounded/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "c63cb7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "control_directories = os.listdir(path=control_log)\n",
    "bounded_directories = os.listdir(path=bounded_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "cd2982a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_df = {'uuid': [],\n",
    "             'used_budget': [],\n",
    "             'max_iou': [],\n",
    "             'exact': [],\n",
    "             'bounded': []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "ab5e86a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".DS_Store is not a directory\n",
      "search_results.json is not a directory\n",
      ".DS_Store is not a directory\n",
      "search_results.json is not a directory\n"
     ]
    }
   ],
   "source": [
    "bounded = False\n",
    "for directory in control_directories:\n",
    "    # Return all files within directory\n",
    "    try:\n",
    "        files = os.listdir(os.path.join(control_log, directory))\n",
    "        for file in files:\n",
    "            if file.endswith('.json'):\n",
    "                with open(os.path.join(control_log, directory, file), 'r') as f:\n",
    "                    f = json.load(f)\n",
    "                    dict_df['uuid'].append(directory)\n",
    "                    dict_df['used_budget'].append(f[-1]['used_budget'])\n",
    "                    dict_df['max_iou'].append(f[-1]['max_iou'])\n",
    "                    dict_df['exact'].append(True if f[-1]['max_iou'] > 0.99999999 else False)\n",
    "                    dict_df['bounded'].append(bounded)\n",
    "    except NotADirectoryError:\n",
    "        print(\"{} is not a directory\".format(directory))\n",
    "        \n",
    "bounded = True\n",
    "for directory in bounded_directories:\n",
    "    # Return all files within directory\n",
    "    try:\n",
    "        files = os.listdir(os.path.join(bounded_log, directory))\n",
    "        for file in files:\n",
    "            if file.endswith('.json'):\n",
    "                with open(os.path.join(bounded_log, directory, file), 'r') as f:\n",
    "                    f = json.load(f)\n",
    "                    dict_df['uuid'].append(directory)\n",
    "                    dict_df['used_budget'].append(f[-1]['used_budget'])\n",
    "                    dict_df['max_iou'].append(f[-1]['max_iou'])\n",
    "                    dict_df['exact'].append(True if f[-1]['max_iou'] > 0.99999999 else False)\n",
    "                    dict_df['bounded'].append(bounded)\n",
    "    except NotADirectoryError:\n",
    "        print(\"{} is not a directory\".format(directory))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "c2c6cdf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(dict_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "dc889f56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bounded</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>False</th>\n",
       "      <td>11.0</td>\n",
       "      <td>0.888437</td>\n",
       "      <td>0.298132</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.944104</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>11.0</td>\n",
       "      <td>0.294129</td>\n",
       "      <td>0.373506</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.056893</td>\n",
       "      <td>0.639158</td>\n",
       "      <td>0.943616</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         count      mean       std  min       25%       50%       75%  \\\n",
       "bounded                                                                 \n",
       "False     11.0  0.888437  0.298132  0.0  0.944104  1.000000  1.000000   \n",
       "True      11.0  0.294129  0.373506  0.0  0.000000  0.056893  0.639158   \n",
       "\n",
       "              max  \n",
       "bounded            \n",
       "False    1.000000  \n",
       "True     0.943616  "
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(by='bounded')['max_iou'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "1bc5610c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bounded</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>False</th>\n",
       "      <td>11.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.97775</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>11.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         count  mean      std   min   25%   50%   75%   max\n",
       "bounded                                                    \n",
       "False     11.0   9.0  8.97775   1.0   1.0   5.0  20.0  20.0\n",
       "True      11.0  20.0  0.00000  20.0  20.0  20.0  20.0  20.0"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(by='bounded')['used_budget'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d13b30",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "\n",
    "We tested the new search algorithm against the `search` function in `search_best` on a subset of the test set. Testing on the entire set proved to be a time intensive task, and early results from the subset were not promising. \n",
    "\n",
    "The implementation did not work as expected. Unfortunately, the agent is not able to use the faces of the bounding box to create extrude cuts, so the best we were able to do was an exact negative of the object. The tables above show the maximum `IoU` achieved for the `bounded=True` group was 0.943, so no exact reconstructions were created. Further the mean `IoU` was only $0.29$ as compared to $0.88$ for the original implementation.\n",
    "\n",
    "The figures below show the issue. The bounding box is created, but often the only valid moves left are to cut away the target object."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56dc9864",
   "metadata": {},
   "source": [
    "<p float=\"left\"> \n",
    "  <img src=\"images/bounded_search_1.png\" style=\"display:inline;margin:1px;height:300px\" /> \n",
    "  <img src=\"images/bounded_searach_2.png\" style=\"display:inline;margin:1px;height:300px\" />  \n",
    "  <img src=\"images/bounded_search_3.png\" style=\"display:inline;margin:1px;height:300px\" /> \n",
    "</p>\n",
    "\n",
    "<center> Search with bounded object and operation space reduced to cuts only </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded3d92d",
   "metadata": {},
   "source": [
    "We do believe though that this implementation could work with added layers of complexity that were not available through the original `train` and `search` functionality provided. To replicate this, the process could be something such as the following psuedocode below:\n",
    "\n",
    "1. `set_target(target)`\n",
    "2. `create_bounding_box(target)`\n",
    "3. `inverse_target = intersect_cut_operation() #returns a body that is the volume of the bounding box specifically not within the target`\n",
    "4. `set_target(inverse_target)`\n",
    "5. `reconstruct(target) #Until IoU=0`\n",
    "6. `inverse the steps found in reconstruct #joins become cuts and vice versa`\n",
    "7. `Insert bounding box as first step`\n",
    "\n",
    "If this was run in parallel or sequentially with the additive search, we may be able to improve the exact reconstruction percent of the search features. However, we were not able to implement this function with the tools provided"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94e7512",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "We attempted to improve the performance of the Fusion 360 Reconstruction Learning Network by improving the MPN with which Q scores for the agent to retrieve were stored, as well as the search algorithm used by the agent to attempt several passes through the search space to find a solution.\n",
    "\n",
    "The augmentation of the MPN resulted in modest improvements to train and test loss and accuracies, while the improvements to the search algorithm proved to be a failure, though we are confident there are ways to jump of the work provided to improve the performance further. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2bf85da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "final_project_env",
   "language": "python",
   "name": "final_project_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
